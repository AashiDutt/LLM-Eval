# LLM Judge Bias Evaluation Framework

A comprehensive framework to test whether AI judges show self-preference bias when evaluating answers from different LLMs.

## ğŸ¯ Research Questions

1. **Self-Preference Bias**: Does a judge favor its own responses?
2. **Domain Effects**: Does bias vary by benchmark/task type?
3. **Hinting Effects**: Does revealing model names change judge behavior?
4. **Tier Preferences**: Do judges favor thinking-tier over fast-tier?
5. **Family Loyalty**: Does a judge favor its model family?

## ğŸ“ Project Structure

```
LLM_Eval/
â”œâ”€â”€ src/                          # Core framework code
â”‚   â”œâ”€â”€ models.py                 # API wrappers (Claude, GPT via OpenRouter, Gemini)
â”‚   â”œâ”€â”€ generate_answers.py       # Generate answers from all models
â”‚   â”œâ”€â”€ judge_answers.py          # Blind judging system
â”‚   â”œâ”€â”€ analysis.py               # Analysis functions
â”‚   â””â”€â”€ utils.py                  # Utilities
â”œâ”€â”€ experiments/                  # Individual experiments
â”‚   â””â”€â”€ exp1_blind_judge/         # âœ… Completed
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.yaml
â”‚       â”œâ”€â”€ prompts.json
â”‚       â”œâ”€â”€ analysis.ipynb
â”‚       â””â”€â”€ data/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§ª Experiments

| # | Experiment | Question | Status |
|---|------------|----------|--------|
| 1 | [Blind Judge](experiments/exp1_blind_judge/) | Self-preference with anonymous answers? | âœ… Done |
| 2 | Benchmark Analysis | Does bias vary by domain? | âœ… Done |
| 3 | Hinting Effect | Does revealing model names matter? | âœ… Done|
| 4 | Fast vs Thinking | Tier preference patterns? | âœ… Done |
| 5 | Family Loyalty | Same-vendor preference? | âœ… Done |

## ğŸš€ Quick Start

### Setup
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Configure API Keys
Create `.env` file:
```bash
ANTHROPIC_API_KEY=your_key
OPENROUTER_API_KEY=your_key
GOOGLE_API_KEY=your_key
```

### Run an Experiment
```bash
# Generate answers
python src/generate_answers.py \
  --config experiments/exp1_blind_judge/config.yaml \
  --prompts experiments/exp1_blind_judge/prompts.json \
  --output experiments/exp1_blind_judge/data/answers/answers.json

# Judge answers
python src/judge_answers.py \
  --config experiments/exp1_blind_judge/config.yaml \
  --answers experiments/exp1_blind_judge/data/answers/answers.json \
  --judges gemini_thinking claude_thinking gpt_thinking

# Analyze
jupyter notebook experiments/exp1_blind_judge/analysis.ipynb
```

## ğŸ”‘ Key Finding (Experiment 1)

| Judge | Picks Own Vendor | Others Pick Same | Self-Bias? |
|-------|------------------|------------------|------------|
| Gemini | 20% | 20% (Claude) | âŒ No |
| Claude | 60% | 60% (Gemini agrees) | âŒ No |
| **GPT** | **80%** | 20% | âš ï¸ **+60% bias** |

**GPT shows strong self-preference bias** (+60% vs other judges).  
Claude's 60% win rate is due to quality, not bias (Gemini judge agrees).

## ğŸ“Š Models Supported

| Vendor | Fast Tier | Thinking Tier |
|--------|-----------|---------------|
| Claude | Haiku 4.5 | Sonnet 4.5 |
| GPT | GPT-5-mini | GPT-5.2 |
| Gemini | 2.5 Flash | 3 Pro |

## ğŸ“„ License

MIT License
