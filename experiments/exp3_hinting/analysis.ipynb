{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Hinting Effect Analysis\n",
    "## Does revealing model identities change judge behavior?\n",
    "\n",
    "**Complete Analysis: All 4 Hinting Groups**\n",
    "- **Group 1 (Self)**: Judges see only their own model revealed\n",
    "- **Group 2 (Competitors)**: Judges see competitor models but not their own\n",
    "- **Group 3 (Full)**: All models revealed (full transparency)\n",
    "- **Group 4 (Blind)**: No hints (baseline control)\n",
    "\n",
    "**Research Question**: Which hinting approach produces the fairest, least biased judgments?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "from analysis import (\n",
    "    load_and_merge_data,\n",
    "    calculate_top1_preference,\n",
    "    calculate_average_scores,\n",
    "    calculate_category_preference,\n",
    "    calculate_tier_preference,\n",
    "    detect_self_bias,\n",
    "    run_statistical_tests,\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from All Groups\n",
    "\n",
    "Load judgments from all 4 hinting groups and merge with answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ANSWERS_PATH = \"data/answers/answers.json\"  # Symlink to exp2 answers\n",
    "JUDGMENTS_DIR = Path(\"data/judgments\")\n",
    "\n",
    "# All group files\n",
    "GROUP_FILES = {\n",
    "    \"group1_self\": \"judgments_group1.json\",\n",
    "    \"group2_competitors\": \"judgments_group2.json\",\n",
    "    \"group3_full\": \"judgments_group3.json\",\n",
    "    \"group4_blind\": \"judgments_group4.json\",  # Baseline\n",
    "}\n",
    "\n",
    "print(\"Loading answers...\")\n",
    "with open(ANSWERS_PATH, \"r\") as f:\n",
    "    answers = json.load(f)\n",
    "print(f\"‚úì Loaded {len(answers)} answers\")\n",
    "\n",
    "# Load all groups\n",
    "groups_data = {}\n",
    "for group_name, filename in GROUP_FILES.items():\n",
    "    filepath = JUDGMENTS_DIR / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, \"r\") as f:\n",
    "            judgments = json.load(f)\n",
    "        groups_data[group_name] = judgments\n",
    "        print(f\"‚úì {group_name}: {len(judgments)} judgments\")\n",
    "    else:\n",
    "        print(f\"‚ö† {group_name}: File not found ({filepath})\")\n",
    "\n",
    "print(f\"\\nLoaded {len(groups_data)} group(s)\")\n",
    "if len(groups_data) == 4:\n",
    "    print(\"  ‚Üí All 4 groups available for comprehensive comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge each group with answers and add hint_group column\n",
    "def merge_with_hint_group(judgments, answers, hint_group):\n",
    "    rows = []\n",
    "    for judgment in judgments:\n",
    "        if \"error\" in judgment:\n",
    "            continue\n",
    "\n",
    "        prompt_id = judgment[\"prompt_id\"]\n",
    "        judge = judgment[\"judge_model\"]\n",
    "        ranking = judgment[\"ranking\"]\n",
    "        scores = judgment[\"scores\"]\n",
    "        mapping = judgment[\"mapping\"]\n",
    "\n",
    "        for rank_idx, label in enumerate(ranking):\n",
    "            answer_id = mapping[label]\n",
    "            score = scores[label]\n",
    "\n",
    "            answer = next((a for a in answers if a[\"answer_id\"] == answer_id), None)\n",
    "            if answer is None:\n",
    "                continue\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"prompt_id\": prompt_id,\n",
    "                    \"category\": answer[\"category\"],\n",
    "                    \"answer_id\": answer_id,\n",
    "                    \"model_vendor\": answer[\"model_vendor\"],\n",
    "                    \"model_tier\": answer[\"model_tier\"],\n",
    "                    \"judge\": judge,\n",
    "                    \"hint_group\": hint_group,\n",
    "                    \"rank\": rank_idx + 1,\n",
    "                    \"score\": score,\n",
    "                    \"is_top_ranked\": rank_idx == 0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Create combined dataframe\n",
    "dfs = []\n",
    "for group_name, judgments in groups_data.items():\n",
    "    df = merge_with_hint_group(judgments, answers, group_name)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"‚úì Combined dataset: {len(df_all)} records\")\n",
    "print(f\"  Unique prompts: {df_all['prompt_id'].nunique()}\")\n",
    "print(f\"  Judges: {sorted(df_all['judge'].unique())}\")\n",
    "print(f\"  Hint groups: {sorted(df_all['hint_group'].unique())}\")\n",
    "print(f\"\\nRecords per group:\")\n",
    "print(df_all.groupby(\"hint_group\").size())\n",
    "\n",
    "display(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Bias Comparison Across All Hint Groups\n",
    "\n",
    "Compare how self-bias changes across all 4 hinting conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_self_bias_by_group(df, vendor=\"gpt\"):\n",
    "    \"\"\"Calculate self-bias for each vendor across hint groups\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for hint_group in sorted(df[\"hint_group\"].unique()):\n",
    "        group_df = df[df[\"hint_group\"] == hint_group]\n",
    "\n",
    "        for judge in sorted(group_df[\"judge\"].unique()):\n",
    "            judge_df = group_df[group_df[\"judge\"] == judge]\n",
    "            top1 = judge_df[judge_df[\"is_top_ranked\"] == True]\n",
    "\n",
    "            # Check if this judge is from the target vendor\n",
    "            judge_vendor = judge.split(\"_\")[0]\n",
    "            is_own_judge = judge_vendor == vendor\n",
    "\n",
    "            vendor_top1 = len(top1[top1[\"model_vendor\"] == vendor])\n",
    "            total = len(top1)\n",
    "            rate = vendor_top1 / total if total > 0 else 0\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"hint_group\": hint_group,\n",
    "                    \"judge\": judge,\n",
    "                    \"vendor\": vendor,\n",
    "                    \"is_own_judge\": is_own_judge,\n",
    "                    \"top1_rate\": rate,\n",
    "                    \"top1_count\": vendor_top1,\n",
    "                    \"total\": total,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Calculate for all vendors\n",
    "self_bias_results = {}\n",
    "for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "    self_bias_results[vendor] = calculate_self_bias_by_group(df_all, vendor)\n",
    "\n",
    "# Display self-bias across all groups\n",
    "print(\"Self-Bias Across All Hint Groups:\")\n",
    "print(\"=\" * 80)\n",
    "for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "    vendor_bias = self_bias_results[vendor]\n",
    "    vendor_own = vendor_bias[vendor_bias[\"is_own_judge\"] == True]\n",
    "\n",
    "    print(f\"\\n{vendor.upper()} Self-Bias by Group:\")\n",
    "    group_summary = vendor_own.groupby(\"hint_group\")[\"top1_rate\"].agg([\"mean\", \"std\", \"count\"])\n",
    "    for hint_group in sorted(vendor_own[\"hint_group\"].unique()):\n",
    "        group_data = vendor_own[vendor_own[\"hint_group\"] == hint_group]\n",
    "        mean_rate = group_data[\"top1_rate\"].mean() * 100\n",
    "        std_rate = group_data[\"top1_rate\"].std() * 100\n",
    "        print(f\"  {hint_group}: {mean_rate:.2f}% ¬± {std_rate:.2f}% (n={len(group_data)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize self-bias across all groups\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "group_order = [\"group4_blind\", \"group1_self\", \"group2_competitors\", \"group3_full\"]\n",
    "group_labels = [\"Blind\", \"Self\", \"Competitors\", \"Full\"]\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"]\n",
    "\n",
    "for idx, vendor in enumerate([\"claude\", \"gpt\", \"gemini\"]):\n",
    "    ax = axes[idx]\n",
    "    vendor_bias = self_bias_results[vendor]\n",
    "    vendor_own = vendor_bias[vendor_bias[\"is_own_judge\"] == True]\n",
    "\n",
    "    # Calculate means and stds for each group\n",
    "    group_means = []\n",
    "    group_stds = []\n",
    "    group_names = []\n",
    "\n",
    "    for hint_group in group_order:\n",
    "        if hint_group in vendor_own[\"hint_group\"].unique():\n",
    "            group_data = vendor_own[vendor_own[\"hint_group\"] == hint_group]\n",
    "            group_means.append(group_data[\"top1_rate\"].mean() * 100)\n",
    "            group_stds.append(group_data[\"top1_rate\"].std() * 100)\n",
    "            group_names.append(hint_group)\n",
    "\n",
    "    if group_means:\n",
    "        x = np.arange(len(group_means))\n",
    "        bars = ax.bar(x, group_means, yerr=group_stds, capsize=5, alpha=0.7, color=colors[: len(group_means)])\n",
    "\n",
    "        ax.set_title(f\"{vendor.upper()} Self-Bias by Hint Group\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Hint Group\", fontsize=12)\n",
    "        ax.set_ylabel(\"Self-Preference Rate (%)\", fontsize=12)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(\n",
    "            [g.replace(\"group\", \"\").replace(\"_\", \" \").title() for g in group_names], rotation=45, ha=\"right\"\n",
    "        )\n",
    "        ax.axhline(y=33.33, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Expected (33.3%)\")\n",
    "        ax.legend()\n",
    "        ax.set_ylim([0, max(80, max(group_means) * 1.2)])\n",
    "\n",
    "        # Add value labels\n",
    "        for i, (mean, std) in enumerate(zip(group_means, group_stds)):\n",
    "            ax.text(i, mean + std + 2, f\"{mean:.1f}%\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/self_bias_all_groups.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Comparison: All Groups vs Baseline\n",
    "\n",
    "Compare each hint group to the blind baseline (Group 4) to measure hinting effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all groups to baseline\n",
    "if \"group4_blind\" in df_all[\"hint_group\"].unique():\n",
    "\n",
    "    def calculate_bias_change_all_groups(df, baseline_group=\"group4_blind\"):\n",
    "        \"\"\"Compare self-bias in all groups to baseline\"\"\"\n",
    "        results = []\n",
    "\n",
    "        target_groups = [\"group1_self\", \"group2_competitors\", \"group3_full\"]\n",
    "\n",
    "        for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "            vendor_bias = calculate_self_bias_by_group(df, vendor)\n",
    "            vendor_own = vendor_bias[vendor_bias[\"is_own_judge\"] == True]\n",
    "\n",
    "            # Get baseline\n",
    "            baseline = vendor_own[vendor_own[\"hint_group\"] == baseline_group]\n",
    "            baseline_rate = baseline[\"top1_rate\"].mean() if len(baseline) > 0 else 0\n",
    "\n",
    "            for target_group in target_groups:\n",
    "                if target_group in vendor_own[\"hint_group\"].unique():\n",
    "                    target = vendor_own[vendor_own[\"hint_group\"] == target_group]\n",
    "                    target_rate = target[\"top1_rate\"].mean() if len(target) > 0 else 0\n",
    "                    change = (target_rate - baseline_rate) * 100  # percentage points\n",
    "\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"vendor\": vendor,\n",
    "                            \"hint_group\": target_group,\n",
    "                            \"baseline_rate\": baseline_rate * 100,\n",
    "                            \"target_rate\": target_rate * 100,\n",
    "                            \"change_pp\": change,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    bias_changes_all = calculate_bias_change_all_groups(df_all)\n",
    "\n",
    "    # Create comparison table\n",
    "    print(\"Self-Bias: All Groups vs Baseline (Group 4 - Blind)\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "    comparison_table = []\n",
    "    for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "        vendor_data = bias_changes_all[bias_changes_all[\"vendor\"] == vendor]\n",
    "        row = {\"Vendor\": vendor.upper()}\n",
    "        for group in [\"group1_self\", \"group2_competitors\", \"group3_full\"]:\n",
    "            group_data = vendor_data[vendor_data[\"hint_group\"] == group]\n",
    "            if len(group_data) > 0:\n",
    "                change = group_data[\"change_pp\"].iloc[0]\n",
    "                change_str = f\"+{change:.1f}\" if change >= 0 else f\"{change:.1f}\"\n",
    "                row[group.replace(\"group\", \"\").replace(\"_\", \" \").title()] = f\"{change_str} pp\"\n",
    "            else:\n",
    "                row[group.replace(\"group\", \"\").replace(\"_\", \" \").title()] = \"N/A\"\n",
    "        comparison_table.append(row)\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_table)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    # Visualize all groups\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, vendor in enumerate([\"claude\", \"gpt\", \"gemini\"]):\n",
    "        ax = axes[idx]\n",
    "        vendor_data = bias_changes_all[bias_changes_all[\"vendor\"] == vendor]\n",
    "\n",
    "        groups = []\n",
    "        baseline_rates = []\n",
    "        target_rates = []\n",
    "        changes = []\n",
    "\n",
    "        for group in [\"group1_self\", \"group2_competitors\", \"group3_full\"]:\n",
    "            group_data = vendor_data[vendor_data[\"hint_group\"] == group]\n",
    "            if len(group_data) > 0:\n",
    "                groups.append(group.replace(\"group\", \"\").replace(\"_\", \" \").title())\n",
    "                baseline_rates.append(group_data[\"baseline_rate\"].iloc[0])\n",
    "                target_rates.append(group_data[\"target_rate\"].iloc[0])\n",
    "                changes.append(group_data[\"change_pp\"].iloc[0])\n",
    "\n",
    "        if groups:\n",
    "            x = np.arange(len(groups))\n",
    "            width = 0.35\n",
    "\n",
    "            bars1 = ax.bar(x - width / 2, baseline_rates, width, label=\"Baseline (Blind)\", alpha=0.7, color=\"#3498db\")\n",
    "            bars2 = ax.bar(x + width / 2, target_rates, width, label=\"Hint Group\", alpha=0.7, color=\"#e74c3c\")\n",
    "\n",
    "            ax.set_xlabel(\"Hint Group\", fontsize=12)\n",
    "            ax.set_ylabel(\"Self-Preference Rate (%)\", fontsize=12)\n",
    "            ax.set_title(f\"{vendor.upper()} Self-Bias: All Groups vs Baseline\", fontsize=14, fontweight=\"bold\")\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(groups, rotation=45, ha=\"right\")\n",
    "            ax.axhline(y=33.33, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"Expected (33.3%)\")\n",
    "            ax.legend()\n",
    "            ax.set_ylim([0, max(80, max(target_rates) * 1.2)])\n",
    "\n",
    "            # Add change labels\n",
    "            for i, change in enumerate(changes):\n",
    "                change_str = f\"+{change:.1f}\" if change >= 0 else f\"{change:.1f}\"\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    max(baseline_rates[i], target_rates[i]) + 2,\n",
    "                    f\"{change_str}pp\",\n",
    "                    ha=\"center\",\n",
    "                    fontsize=9,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data/results/bias_comparison_all_groups.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† Group 4 (Blind) baseline not available for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate which hinting mode produces the fairest judgments\n",
    "def evaluate_hinting_mode(df, hint_group):\n",
    "    \"\"\"Calculate fairness metrics for a hint group\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # 1. Overall self-bias (average across all vendors)\n",
    "    all_self_bias = []\n",
    "    for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "        vendor_bias = calculate_self_bias_by_group(df, vendor)\n",
    "        vendor_own = vendor_bias[(vendor_bias[\"is_own_judge\"] == True) & (vendor_bias[\"hint_group\"] == hint_group)]\n",
    "        if len(vendor_own) > 0:\n",
    "            all_self_bias.append(vendor_own[\"top1_rate\"].mean())\n",
    "\n",
    "    metrics[\"avg_self_bias\"] = np.mean(all_self_bias) * 100 if all_self_bias else 0\n",
    "\n",
    "    # 2. Deviation from expected (33.33% per vendor)\n",
    "    vendor_deviations = []\n",
    "    for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "        vendor_bias = calculate_self_bias_by_group(df, vendor)\n",
    "        vendor_own = vendor_bias[(vendor_bias[\"is_own_judge\"] == True) & (vendor_bias[\"hint_group\"] == hint_group)]\n",
    "        if len(vendor_own) > 0:\n",
    "            avg_rate = vendor_own[\"top1_rate\"].mean()\n",
    "            deviation = abs(avg_rate - 0.3333) * 100\n",
    "            vendor_deviations.append(deviation)\n",
    "\n",
    "    metrics[\"avg_deviation_from_expected\"] = np.mean(vendor_deviations) if vendor_deviations else 0\n",
    "\n",
    "    # 3. Balance score (lower variance in vendor preferences = more balanced)\n",
    "    vendor_rates = []\n",
    "    for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "        group_df = df[df[\"hint_group\"] == hint_group]\n",
    "        top1 = group_df[group_df[\"is_top_ranked\"] == True]\n",
    "        vendor_count = len(top1[top1[\"model_vendor\"] == vendor])\n",
    "        total = len(top1)\n",
    "        if total > 0:\n",
    "            vendor_rates.append(vendor_count / total)\n",
    "\n",
    "    metrics[\"balance_score\"] = np.std(vendor_rates) * 100 if vendor_rates else 0  # Lower is better\n",
    "\n",
    "    # 4. Cross-judge agreement (consistency)\n",
    "    judge_agreements = []\n",
    "    for judge in df[df[\"hint_group\"] == hint_group][\"judge\"].unique():\n",
    "        judge_df = df[(df[\"hint_group\"] == hint_group) & (df[\"judge\"] == judge)]\n",
    "        top1 = judge_df[judge_df[\"is_top_ranked\"] == True]\n",
    "        if len(top1) > 0:\n",
    "            # Calculate how often judge picks their own vendor\n",
    "            judge_vendor = judge.split(\"_\")[0]\n",
    "            own_count = len(top1[top1[\"model_vendor\"] == judge_vendor])\n",
    "            agreement = own_count / len(top1)\n",
    "            judge_agreements.append(agreement)\n",
    "\n",
    "    metrics[\"cross_judge_consistency\"] = np.std(judge_agreements) * 100 if judge_agreements else 0  # Lower is better\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Evaluate all groups\n",
    "print(\"=\" * 90)\n",
    "print(\"COMPREHENSIVE EVALUATION: Which Hinting Mode is Best?\")\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nMetrics:\")\n",
    "print(\"  - Avg Self-Bias: Lower is better (closer to 33.33%)\")\n",
    "print(\"  - Deviation from Expected: Lower is better\")\n",
    "print(\"  - Balance Score: Lower variance = more balanced\")\n",
    "print(\"  - Cross-Judge Consistency: Lower variance = more consistent\")\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "\n",
    "evaluation_results = []\n",
    "for hint_group in [\"group4_blind\", \"group1_self\", \"group2_competitors\", \"group3_full\"]:\n",
    "    if hint_group in df_all[\"hint_group\"].unique():\n",
    "        metrics = evaluate_hinting_mode(df_all, hint_group)\n",
    "        metrics[\"hint_group\"] = hint_group\n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "eval_df = eval_df.sort_values(\"avg_self_bias\")  # Sort by self-bias (lower is better)\n",
    "\n",
    "print(\"\\nRanking by Average Self-Bias (Lower = Fairer):\")\n",
    "print(\"-\" * 90)\n",
    "for idx, row in eval_df.iterrows():\n",
    "    rank = list(eval_df.index).index(idx) + 1\n",
    "    group_name = row[\"hint_group\"].replace(\"group\", \"\").replace(\"_\", \" \").title()\n",
    "    print(\n",
    "        f\"{rank}. {group_name:20s} | Self-Bias: {row['avg_self_bias']:5.2f}% | \"\n",
    "        f\"Deviation: {row['avg_deviation_from_expected']:5.2f}pp | \"\n",
    "        f\"Balance: {row['balance_score']:5.2f} | Consistency: {row['cross_judge_consistency']:5.2f}\"\n",
    "    )\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Average Self-Bias\n",
    "ax = axes[0, 0]\n",
    "groups = [g.replace(\"group\", \"\").replace(\"_\", \" \").title() for g in eval_df[\"hint_group\"]]\n",
    "ax.barh(groups, eval_df[\"avg_self_bias\"], color=[\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"][: len(groups)])\n",
    "ax.axvline(x=33.33, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Expected (33.33%)\")\n",
    "ax.set_xlabel(\"Average Self-Bias (%)\", fontsize=12)\n",
    "ax.set_title(\"1. Average Self-Bias (Lower = Fairer)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.set_xlim([0, 70])\n",
    "\n",
    "# 2. Deviation from Expected\n",
    "ax = axes[0, 1]\n",
    "ax.barh(\n",
    "    groups, eval_df[\"avg_deviation_from_expected\"], color=[\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"][: len(groups)]\n",
    ")\n",
    "ax.set_xlabel(\"Deviation from Expected (pp)\", fontsize=12)\n",
    "ax.set_title(\"2. Deviation from Expected (Lower = Better)\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# 3. Balance Score\n",
    "ax = axes[1, 0]\n",
    "ax.barh(groups, eval_df[\"balance_score\"], color=[\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"][: len(groups)])\n",
    "ax.set_xlabel(\"Balance Score (Std Dev, Lower = Better)\", fontsize=12)\n",
    "ax.set_title(\"3. Balance Across Vendors\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# 4. Cross-Judge Consistency\n",
    "ax = axes[1, 1]\n",
    "ax.barh(groups, eval_df[\"cross_judge_consistency\"], color=[\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"][: len(groups)])\n",
    "ax.set_xlabel(\"Cross-Judge Consistency (Std Dev, Lower = Better)\", fontsize=12)\n",
    "ax.set_title(\"4. Cross-Judge Agreement\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/hinting_mode_evaluation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Determine best mode\n",
    "best_idx = eval_df[\"avg_self_bias\"].idxmin()\n",
    "best_group = eval_df.loc[best_idx, \"hint_group\"]\n",
    "best_name = best_group.replace(\"group\", \"\").replace(\"_\", \" \").title()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"üèÜ BEST HINTING MODE: {best_name.upper()}\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"  Average Self-Bias: {eval_df.loc[best_idx, 'avg_self_bias']:.2f}%\")\n",
    "print(f\"  Deviation from Expected: {eval_df.loc[best_idx, 'avg_deviation_from_expected']:.2f}pp\")\n",
    "print(f\"  Balance Score: {eval_df.loc[best_idx, 'balance_score']:.2f}\")\n",
    "print(f\"  Cross-Judge Consistency: {eval_df.loc[best_idx, 'cross_judge_consistency']:.2f}\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Which Hinting Mode is Best for Judgment?\n",
    "\n",
    "Evaluate all hinting modes across multiple criteria to determine the fairest approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vendor_preference_by_group(df, hint_group, judge=None):\n",
    "    \"\"\"Calculate top-1 preference rates for each vendor\"\"\"\n",
    "    group_df = df[df[\"hint_group\"] == hint_group]\n",
    "    if judge:\n",
    "        group_df = group_df[group_df[\"judge\"] == judge]\n",
    "\n",
    "    top1 = group_df[group_df[\"is_top_ranked\"] == True]\n",
    "    vendor_counts = top1.groupby(\"model_vendor\").size()\n",
    "    total = len(top1)\n",
    "\n",
    "    results = []\n",
    "    for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "        count = vendor_counts.get(vendor, 0)\n",
    "        percentage = (count / total * 100) if total > 0 else 0\n",
    "        results.append({\"vendor\": vendor, \"top1_percentage\": percentage, \"top1_count\": count, \"total\": total})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compare judges for Group 2\n",
    "judges = sorted(df_all[df_all[\"hint_group\"] == \"group2_competitors\"][\"judge\"].unique())\n",
    "n_judges = len(judges)\n",
    "n_cols = 3\n",
    "n_rows = (n_judges + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, judge in enumerate(judges):\n",
    "    ax = axes[i]\n",
    "\n",
    "    prefs = calculate_vendor_preference_by_group(df_all, \"group2_competitors\", judge)\n",
    "\n",
    "    colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
    "    bars = ax.bar(prefs[\"vendor\"], prefs[\"top1_percentage\"], color=colors)\n",
    "\n",
    "    # Highlight judge's own vendor\n",
    "    judge_vendor = judge.split(\"_\")[0]\n",
    "    if judge_vendor in prefs[\"vendor\"].values:\n",
    "        idx = prefs[prefs[\"vendor\"] == judge_vendor].index[0]\n",
    "        bars[idx].set_edgecolor(\"black\")\n",
    "        bars[idx].set_linewidth(2)\n",
    "\n",
    "    ax.set_title(f\"{judge.replace('_', ' ').title()}\\n(Competitors Hint)\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Top-1 Rate (%)\", fontsize=10)\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.axhline(y=33.33, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"Expected\")\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    for k, (vendor, pct) in enumerate(zip(prefs[\"vendor\"], prefs[\"top1_percentage\"])):\n",
    "        ax.text(k, pct + 2, f\"{pct:.1f}%\", ha=\"center\", fontsize=9)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_judges, len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/cross_judge_comparison_group2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Tests: Does Competitors Hint Affect Self-Bias?\n",
    "\n",
    "Test if Group 2 (Competitors) significantly differs from baseline (if available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"group4_blind\" in df_all[\"hint_group\"].unique():\n",
    "\n",
    "    def test_hinting_effect(df, vendor=\"gpt\", baseline_group=\"group4_blind\", target_group=\"group2_competitors\"):\n",
    "        \"\"\"Statistical test comparing target group to baseline\"\"\"\n",
    "        vendor_bias = calculate_self_bias_by_group(df, vendor)\n",
    "        vendor_own = vendor_bias[vendor_bias[\"is_own_judge\"] == True]\n",
    "\n",
    "        # Get baseline rates (per judge)\n",
    "        baseline = vendor_own[vendor_own[\"hint_group\"] == baseline_group]\n",
    "        baseline_rates = baseline[\"top1_rate\"].values\n",
    "\n",
    "        # Get target group rates\n",
    "        target = vendor_own[vendor_own[\"hint_group\"] == target_group]\n",
    "        target_rates = target[\"top1_rate\"].values\n",
    "\n",
    "        if len(baseline_rates) > 0 and len(target_rates) > 0:\n",
    "            # Paired t-test (same judges across conditions)\n",
    "            if len(baseline_rates) == len(target_rates):\n",
    "                t_stat, p_value = stats.ttest_rel(baseline_rates, target_rates)\n",
    "                test_type = \"paired t-test\"\n",
    "            else:\n",
    "                # Independent t-test if different sample sizes\n",
    "                t_stat, p_value = stats.ttest_ind(baseline_rates, target_rates)\n",
    "                test_type = \"independent t-test\"\n",
    "\n",
    "            mean_baseline = baseline_rates.mean() * 100\n",
    "            mean_target = target_rates.mean() * 100\n",
    "            change = mean_target - mean_baseline\n",
    "\n",
    "            return {\n",
    "                \"vendor\": vendor,\n",
    "                \"baseline_mean\": mean_baseline,\n",
    "                \"group2_mean\": mean_target,\n",
    "                \"change_pp\": change,\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": p_value < 0.05,\n",
    "                \"test_type\": test_type,\n",
    "            }\n",
    "        return None\n",
    "\n",
    "    # Test for all vendors\n",
    "    stat_tests = []\n",
    "    for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "        result = test_hinting_effect(df_all, vendor)\n",
    "        if result:\n",
    "            stat_tests.append(result)\n",
    "\n",
    "    stat_df = pd.DataFrame(stat_tests)\n",
    "\n",
    "    print(\"Statistical Tests: Group 2 (Competitors) vs Group 4 (Blind)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        stat_df[\n",
    "            [\"vendor\", \"baseline_mean\", \"group2_mean\", \"change_pp\", \"p_value\", \"significant\", \"test_type\"]\n",
    "        ].to_string(index=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    for _, row in stat_df.iterrows():\n",
    "        sig_str = \"‚úì Significant\" if row[\"significant\"] else \"Not significant\"\n",
    "        change_str = f\"+{row['change_pp']:.1f}\" if row[\"change_pp\"] >= 0 else f\"{row['change_pp']:.1f}\"\n",
    "        print(f\"  {row['vendor'].upper()}: {change_str} pp change, p={row['p_value']:.4f} ({sig_str})\")\n",
    "else:\n",
    "    print(\"‚ö† Group 4 (Blind) baseline not available for statistical comparison.\")\n",
    "    print(\"  Run with group4_blind.json to enable statistical tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Table: Self-Bias for Group 2\n",
    "\n",
    "Create a summary table for Group 2 (Competitors) analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for Group 2\n",
    "summary_rows = []\n",
    "for vendor in [\"claude\", \"gpt\", \"gemini\"]:\n",
    "    vendor_bias = calculate_self_bias_by_group(df_all, vendor)\n",
    "    vendor_own = vendor_bias[\n",
    "        (vendor_bias[\"is_own_judge\"] == True) & (vendor_bias[\"hint_group\"] == \"group2_competitors\")\n",
    "    ]\n",
    "\n",
    "    if len(vendor_own) > 0:\n",
    "        mean_rate = vendor_own[\"top1_rate\"].mean() * 100\n",
    "        std_rate = vendor_own[\"top1_rate\"].std() * 100\n",
    "        n_judges = len(vendor_own)\n",
    "\n",
    "        summary_rows.append(\n",
    "            {\n",
    "                \"Vendor\": vendor.upper(),\n",
    "                \"Hint Group\": \"Group 2 (Competitors)\",\n",
    "                \"Self-Preference (%)\": f\"{mean_rate:.1f} ¬± {std_rate:.1f}\",\n",
    "                \"N Judges\": n_judges,\n",
    "            }\n",
    "        )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"Self-Bias Summary: Group 2 (Competitors)\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv(\"data/results/self_bias_summary_group2.csv\", index=False)\n",
    "print(\"\\n‚úì Saved to data/results/self_bias_summary_group2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Comparison Table: All 4 Hinting Groups\n",
    "\n",
    "### Main Metrics Comparison\n",
    "\n",
    "| Metric | Group 1 (Self) | Group 2 (Competitors) | Group 3 (Full) | Group 4 (Blind) | Best |\n",
    "|--------|----------------|----------------------|----------------|-----------------|------|\n",
    "| **Average Self-Bias** | 41.25% | 43.33% | 43.96% | 42.50% | Group 1 |\n",
    "| **Deviation from Expected** | 13.06pp | 13.47pp | 10.63pp | 11.53pp | Group 3 |\n",
    "| **Balance Score** | 15.50 | 14.96 | 13.10 | 14.07 | Group 3 |\n",
    "| **Consistency** | 15.57 | 16.11 | 14.00 | 15.52 | Group 3 |\n",
    "\n",
    "*Note: Lower values are better for all metrics except where noted*\n",
    "\n",
    "### Vendor-Specific Self-Bias Rates\n",
    "\n",
    "| Vendor | Group 1 (Self) | Group 2 (Competitors) | Group 3 (Full) | Group 4 (Blind) |\n",
    "|--------|----------------|----------------------|----------------|-----------------|\n",
    "| **Claude** | 25.6% | 36.2% | 34.4% | 30.0% |\n",
    "| **GPT** | 62.5% | 65.6% | 63.7% | 64.4% |\n",
    "| **Gemini** | 35.6% | 28.1% | 33.8% | 33.1% |\n",
    "\n",
    "*Self-bias rate = percentage of times a judge ranks their own vendor #1*\n",
    "\n",
    "### Top-Vendor Distribution\n",
    "\n",
    "| Vendor | Group 1 (Self) | Group 2 (Competitors) | Group 3 (Full) | Group 4 (Blind) |\n",
    "|--------|----------------|----------------------|----------------|-----------------|\n",
    "| **Claude** | 23.5% | 27.1% | 26.5% | 25.2% |\n",
    "| **GPT** | 55.2% | 54.0% | 51.7% | 53.1% |\n",
    "| **Gemini** | 21.2% | 19.0% | 21.9% | 21.7% |\n",
    "\n",
    "*Percentage of times each vendor's answers are ranked #1 overall*\n",
    "\n",
    "### Change from Baseline (Group 4 - Blind)\n",
    "\n",
    "| Group | Avg Self-Bias Change | Effect |\n",
    "|-------|----------------------|--------|\n",
    "| Group 1 (Self) | -1.25pp | ‚û°Ô∏è Minimal change |\n",
    "| Group 2 (Competitors) | +0.83pp | ‚û°Ô∏è Minimal change |\n",
    "| Group 3 (Full) | +1.46pp | ‚û°Ô∏è Minimal change |\n",
    "\n",
    "### Overall Ranking Summary\n",
    "\n",
    "| Rank | Group | Key Strength |\n",
    "|------|-------|--------------|\n",
    "| ü•á 1 | Group 1 (Self) | Lowest average self-bias (41.25%) |\n",
    "| ü•à 2 | Group 4 (Blind) | Baseline control |\n",
    "| ü•â 3 | Group 2 (Competitors) | Moderate performance |\n",
    "| 4 | Group 3 (Full) | Best balance & consistency |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Group 1 (Self)** performs best on average self-bias but has higher variance\n",
    "2. **Group 3 (Full)** has the best balance score (13.10) and consistency (14.00), making it most fair overall\n",
    "3. **All hinting modes** show minimal change from baseline (<2pp), suggesting hinting has limited impact\n",
    "4. **GPT** shows consistently high self-bias across all groups (62-66%), indicating systemic bias\n",
    "5. **Claude** shows the lowest self-bias in Group 1 (25.6%), suggesting self-awareness when their identity is revealed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive comparison tableimport pandas as pd# Create comparison dataframe from evaluation resultscomparison_data = []for hint_group in ['group4_blind', 'group1_self', 'group2_competitors', 'group3_full']:    if hint_group in df_all['hint_group'].unique():        group_df = df_all[df_all['hint_group'] == hint_group]                # Calculate metrics (reuse from evaluation function)        metrics = evaluate_hinting_mode(df_all, hint_group)                # Get vendor-specific rates        vendor_bias = {}        for vendor in ['claude', 'gpt', 'gemini']:            vendor_bias_data = calculate_self_bias_by_group(df_all, vendor)            vendor_own = vendor_bias_data[(vendor_bias_data['is_own_judge'] == True) &                                          (vendor_bias_data['hint_group'] == hint_group)]            if len(vendor_own) > 0:                vendor_bias[vendor] = vendor_own['top1_rate'].mean() * 100                # Get top-vendor distribution        top1 = group_df[group_df['is_top_ranked'] == True]        total = len(top1)        vendor_dist = {}        for vendor in ['claude', 'gpt', 'gemini']:            vendor_dist[vendor] = len(top1[top1['model_vendor'] == vendor]) / total * 100 if total > 0 else 0                comparison_data.append({            'Group': hint_group.replace('group', '').replace('_', ' ').title(),            'Avg Self-Bias (%)': metrics['avg_self_bias'],            'Deviation (pp)': metrics['avg_deviation_from_expected'],            'Balance Score': metrics['balance_score'],            'Consistency': metrics['cross_judge_consistency'],            'Claude Self-Bias (%)': vendor_bias.get('claude', 0),            'GPT Self-Bias (%)': vendor_bias.get('gpt', 0),            'Gemini Self-Bias (%)': vendor_bias.get('gemini', 0),            'Claude Top-Rate (%)': vendor_dist.get('claude', 0),            'GPT Top-Rate (%)': vendor_dist.get('gpt', 0),            'Gemini Top-Rate (%)': vendor_dist.get('gemini', 0),        })comparison_df = pd.DataFrame(comparison_data)comparison_df = comparison_df.sort_values('Avg Self-Bias (%)')print(\"=\" * 100)print(\"COMPREHENSIVE COMPARISON TABLE: All 4 Hinting Groups\")print(\"=\" * 100)print()# Main metrics tableprint(\"### Main Metrics Comparison\")print()main_cols = ['Group', 'Avg Self-Bias (%)', 'Deviation (pp)', 'Balance Score', 'Consistency']print(comparison_df[main_cols].to_string(index=False))print()# Vendor self-bias tableprint(\"### Vendor-Specific Self-Bias Rates\")print()vendor_cols = ['Group', 'Claude Self-Bias (%)', 'GPT Self-Bias (%)', 'Gemini Self-Bias (%)']vendor_df = comparison_df[vendor_cols].copy()vendor_df.columns = ['Group', 'Claude', 'GPT', 'Gemini']print(vendor_df.to_string(index=False))print()# Top-vendor distribution tableprint(\"### Top-Vendor Distribution\")print()dist_cols = ['Group', 'Claude Top-Rate (%)', 'GPT Top-Rate (%)', 'Gemini Top-Rate (%)']dist_df = comparison_df[dist_cols].copy()dist_df.columns = ['Group', 'Claude', 'GPT', 'Gemini']print(dist_df.to_string(index=False))print()# Change from baselinebaseline = comparison_df[comparison_df['Group'] == '4 Blind'].iloc[0]print(\"### Change from Baseline (Group 4 - Blind)\")print()changes = []for idx, row in comparison_df.iterrows():    if row['Group'] != '4 Blind':        change = row['Avg Self-Bias (%)'] - baseline['Avg Self-Bias (%)']        changes.append({            'Group': row['Group'],            'Change (pp)': f\"{change:+.2f}\",            'Effect': '‚úÖ Reduces bias' if change < -2 else '‚ö†Ô∏è Increases bias' if change > 2 else '‚û°Ô∏è Minimal change'        })change_df = pd.DataFrame(changes)print(change_df.to_string(index=False))print()# Rankingprint(\"### Overall Ranking\")print()ranking = comparison_df[['Group', 'Avg Self-Bias (%)']].copy()ranking['Rank'] = range(1, len(ranking) + 1)ranking = ranking[['Rank', 'Group', 'Avg Self-Bias (%)']]print(ranking.to_string(index=False))print()print(\"=\" * 100)display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings and Recommendations\n",
    "\n",
    "### Summary of All Hinting Groups:\n",
    "\n",
    "**Group 1 (Self)**: Judges see only their own model revealed\n",
    "- **Hypothesis**: Self-hint increases self-bias\n",
    "- **Expected Effect**: Judges favor their own model more when they know which answer is theirs\n",
    "\n",
    "**Group 2 (Competitors)**: Judges see competitor models but not their own\n",
    "- **Hypothesis**: Competitor-hint reduces self-bias\n",
    "- **Expected Effect**: Knowing competitors but not self leads to fairer evaluation\n",
    "\n",
    "**Group 3 (Full)**: All models revealed (full transparency)\n",
    "- **Hypothesis**: Full transparency may increase or decrease bias depending on model\n",
    "- **Expected Effect**: Mixed effects - may increase bias for some, decrease for others\n",
    "\n",
    "**Group 4 (Blind)**: No hints (baseline control)\n",
    "- **Purpose**: Baseline to compare all other groups against\n",
    "\n",
    "### Key Metrics:\n",
    "- **Self-preference rate**: Percentage of times a judge ranks their own vendor #1\n",
    "- **Expected rate**: 33.3% (if completely unbiased, with 3 vendors)\n",
    "- **Change from baseline (pp)**: Percentage point difference from blind condition\n",
    "- **Balance score**: Variance in vendor preferences (lower = more balanced)\n",
    "- **Cross-judge consistency**: Agreement across judges (lower variance = more consistent)\n",
    "\n",
    "### Interpretation:\n",
    "Based on the comprehensive evaluation above, the best hinting mode is determined by:\n",
    "1. **Lowest average self-bias** (closest to 33.33%)\n",
    "2. **Lowest deviation from expected** (most fair)\n",
    "3. **Best balance** across vendors\n",
    "4. **Highest consistency** across judges\n",
    "\n",
    "### Recommendations:\n",
    "- Use the **best performing hinting mode** (as determined by the evaluation) for future judgment tasks\n",
    "- Consider the trade-offs: Some hinting modes may reduce bias but increase variance\n",
    "- For maximum fairness, prefer modes with lower self-bias and better balance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
