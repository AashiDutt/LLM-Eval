# Configuration for Multi-Model Bias Evaluation

# API Configuration
api_keys:
  anthropic: ${ANTHROPIC_API_KEY}
  openrouter: ${OPENROUTER_API_KEY} 
  google: ${GOOGLE_API_KEY}

# Model Configuration (using actual model names)
models:
  claude:
    fast: "claude-haiku-4-5"        # Fast tier: Claude Haiku 4.5
    thinking: "claude-sonnet-4-5"   # Thinking tier: Claude Sonnet 4.5

  gpt:
    fast: "gpt-5-mini"       # Fast tier: GPT-5 mini (via OpenRouter)
    thinking: "gpt-5.2"      # Thinking tier: GPT-4.1 (via OpenRouter)

  gemini:
    fast: "gemini-2.5-flash"         # Fast tier (also used for judging)
    thinking: "gemini-3-pro-preview" # Thinking tier for answer generation

# Judge Configuration
judges:
  primary: "gemini_fast"  # Using gemini-2.5-flash for fast, reliable judging
  additional:  # Optional: compare with other judges
    - "claude_thinking"
    - "gpt_thinking"

# Generation Settings
generation:
  temperature: 0.7
  max_tokens: 2048
  timeout: 60  # seconds

# Judging Settings
judging:
  temperature: 0.1  # Very low temperature for consistent JSON output
  max_tokens: 1500  # Enough for JSON response
  shuffle_seed: 42  # For reproducible shuffling
  anonymize: true

# Experiment Settings
experiment:
  prompts_per_category: 30  # Number of prompts per category
  categories:
    - factual_qa
    - reasoning_math
    - coding
    - summarization
    - creative_writing
    - safety_sensitive
  
  # Statistical testing
  significance_level: 0.05
  min_prompts_per_test: 10

# Output Settings
output:
  save_intermediate: true
  verbose: true
  results_dir: "data/results"
  timestamp_format: "%Y%m%d_%H%M%S"
