{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Bias Evaluation Analysis\n",
    "## Does Gemini 3 Favor Its Own Answers?\n",
    "\n",
    "This notebook analyzes the results from our bias evaluation experiment comparing Claude 4.5, GPT 5.1, and Gemini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from analysis import (\n",
    "    load_and_merge_data,\n",
    "    calculate_top1_preference,\n",
    "    calculate_average_scores,\n",
    "    calculate_category_preference,\n",
    "    calculate_tier_preference,\n",
    "    detect_self_bias,\n",
    "    run_statistical_tests,\n",
    "    generate_summary_report,\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Update the file paths below to point to your generated data, or let it automatically find the latest files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the downloaded MT-Bench data from HuggingFace\n",
    "ANSWERS_PATH = \"data/answers/answers_mt_bench.json\"\n",
    "JUDGMENTS_PATH = \"data/judgments/judgments_mt_bench.json\"\n",
    "\n",
    "print(f\"Using answers: {ANSWERS_PATH}\")\n",
    "print(f\"Using judgments: {JUDGMENTS_PATH}\")\n",
    "\n",
    "df = load_and_merge_data(ANSWERS_PATH, JUDGMENTS_PATH)\n",
    "print(f\"\\nLoaded {len(df)} judgment records\")\n",
    "print(f\"Unique prompts: {df['prompt_id'].nunique()}\")\n",
    "print(f\"Judges: {df['judge'].unique().tolist()}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Report\n",
    "\n",
    "Generate overall bias summary for Gemini as judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = generate_summary_report(df, target_vendor=\"gemini\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization: Top-1 Preference Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judges = df[\"judge\"].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(judges), figsize=(6 * len(judges), 5))\n",
    "if len(judges) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, judge in enumerate(judges):\n",
    "    top1_prefs = calculate_top1_preference(df, judge)\n",
    "\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(top1_prefs[\"vendor\"], top1_prefs[\"top1_percentage\"], color=[\"#3498db\", \"#e74c3c\", \"#2ecc71\"])\n",
    "\n",
    "    if judge.endswith(\"_thinking\"):\n",
    "        vendor = judge.split(\"_\")[0]\n",
    "        if vendor in top1_prefs[\"vendor\"].values:\n",
    "            idx = top1_prefs[top1_prefs[\"vendor\"] == vendor].index[0]\n",
    "            bars[idx].set_color(\"#f39c12\")\n",
    "            bars[idx].set_edgecolor(\"black\")\n",
    "            bars[idx].set_linewidth(2)\n",
    "\n",
    "    ax.set_title(f\"Judge: {judge}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Model Vendor\", fontsize=12)\n",
    "    ax.set_ylabel(\"Top-1 Selection Rate (%)\", fontsize=12)\n",
    "    ax.set_ylim([0, max(50, top1_prefs[\"top1_percentage\"].max() * 1.2)])\n",
    "    ax.axhline(y=33.33, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Expected (33.3%)\")\n",
    "    ax.legend()\n",
    "\n",
    "    for j, (vendor, pct) in enumerate(zip(top1_prefs[\"vendor\"], top1_prefs[\"top1_percentage\"])):\n",
    "        ax.text(j, pct + 1, f\"{pct:.1f}%\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/top1_preferences.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Average Scores by Vendor and Tier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_judge = \"gemini_thinking\"\n",
    "scores_df = calculate_average_scores(df, target_judge)\n",
    "\n",
    "# Pivot for visualization\n",
    "pivot = scores_df.pivot(index=\"vendor\", columns=\"tier\", values=\"mean_score\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pivot.plot(kind=\"bar\", ax=ax, color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax.set_title(f\"Average Scores by Vendor and Tier (Judge: {target_judge})\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Model Vendor\", fontsize=12)\n",
    "ax.set_ylabel(\"Average Score (0-10)\", fontsize=12)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "ax.legend(title=\"Tier\", labels=[\"Fast\", \"Thinking\"])\n",
    "ax.set_ylim([0, 10])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/average_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage Scores:\")\n",
    "print(scores_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Category-wise Preference Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_judge = \"gemini_thinking\"\n",
    "category_prefs = calculate_category_preference(df, target_judge)\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot = category_prefs.pivot(index=\"category\", columns=\"model_vendor\", values=\"percentage\")\n",
    "pivot = pivot.fillna(0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".1f\", cmap=\"YlOrRd\", cbar_kws={\"label\": \"Top-1 Selection Rate (%)\"}, ax=ax)\n",
    "ax.set_title(f\"Top-1 Preferences by Category (Judge: {target_judge})\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Model Vendor\", fontsize=12)\n",
    "ax.set_ylabel(\"Category\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/category_preferences.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCategory Preferences:\")\n",
    "print(category_prefs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tier Preference Analysis (Fast vs Thinking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_judge = \"gemini_thinking\"\n",
    "tier_prefs = calculate_tier_preference(df, target_judge)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Group by vendor\n",
    "vendors = tier_prefs[\"model_vendor\"].unique()\n",
    "x = np.arange(len(vendors))\n",
    "width = 0.35\n",
    "\n",
    "fast_data = []\n",
    "thinking_data = []\n",
    "\n",
    "for vendor in vendors:\n",
    "    vendor_data = tier_prefs[tier_prefs[\"model_vendor\"] == vendor]\n",
    "    fast = vendor_data[vendor_data[\"model_tier\"] == \"fast\"][\"percentage\"].values\n",
    "    thinking = vendor_data[vendor_data[\"model_tier\"] == \"thinking\"][\"percentage\"].values\n",
    "    fast_data.append(fast[0] if len(fast) > 0 else 0)\n",
    "    thinking_data.append(thinking[0] if len(thinking) > 0 else 0)\n",
    "\n",
    "ax.bar(x - width / 2, fast_data, width, label=\"Fast\", color=\"#3498db\")\n",
    "ax.bar(x + width / 2, thinking_data, width, label=\"Thinking\", color=\"#e74c3c\")\n",
    "\n",
    "ax.set_title(f\"Tier Preferences by Vendor (Judge: {target_judge})\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Model Vendor\", fontsize=12)\n",
    "ax.set_ylabel(\"Percentage of Vendor's Top-1 Selections\", fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vendors)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 100])\n",
    "ax.axhline(y=50, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/tier_preferences.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTier Preferences:\")\n",
    "print(tier_prefs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Bias Detection\n",
    "\n",
    "Compare how different judges rank each vendor's answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect self-bias for each vendor\n",
    "vendors = [\"claude\", \"gpt\", \"gemini\"]\n",
    "bias_results = {}\n",
    "\n",
    "for vendor in vendors:\n",
    "    bias_results[vendor] = detect_self_bias(df, vendor)\n",
    "\n",
    "    if \"bias_analysis\" in bias_results[vendor]:\n",
    "        ba = bias_results[vendor][\"bias_analysis\"]\n",
    "        print(f\"\\n{vendor.upper()} Self-Bias Analysis:\")\n",
    "        print(f\"  {vendor.upper()} judge ranks {vendor.upper()} #1: {ba['target_judge_rate'] * 100:.2f}%\")\n",
    "        print(f\"  Other judges rank {vendor.upper()} #1: {ba['other_judges_avg_rate'] * 100:.2f}%\")\n",
    "        print(f\"  Bias difference: {ba['bias_percentage_points']:+.2f} percentage points\")\n",
    "\n",
    "        if abs(ba[\"bias_difference\"]) > 0.1:\n",
    "            if ba[\"bias_difference\"] > 0:\n",
    "                print(\"  ⚠️  POTENTIAL SELF-BIAS DETECTED\")\n",
    "            else:\n",
    "                print(\"  ⚠️  POTENTIAL SELF-PENALTY DETECTED\")\n",
    "        else:\n",
    "            print(\"  ✓ No significant self-bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Significance Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run statistical tests for Gemini\n",
    "stat_tests = run_statistical_tests(df, target_vendor=\"gemini\")\n",
    "\n",
    "print(\"Statistical Tests for Gemini Self-Bias:\")\n",
    "print(\"\\n1. Chi-Square Goodness of Fit Test\")\n",
    "print(f\"   Null Hypothesis: {stat_tests['chi_square_test']['null_hypothesis']}\")\n",
    "print(f\"   χ² = {stat_tests['chi_square_test']['chi2_statistic']:.3f}\")\n",
    "print(f\"   p-value = {stat_tests['chi_square_test']['p_value']:.4f}\")\n",
    "print(f\"   Significant at α=0.05: {stat_tests['chi_square_test']['significant_at_0.05']}\")\n",
    "\n",
    "print(\"\\n2. Binomial Test\")\n",
    "print(f\"   Null Hypothesis: {stat_tests['binomial_test']['null_hypothesis']}\")\n",
    "print(f\"   Observed rate: {stat_tests['binomial_test']['observed_rate'] * 100:.2f}%\")\n",
    "print(f\"   Expected rate: {stat_tests['binomial_test']['expected_rate'] * 100:.2f}%\")\n",
    "print(f\"   p-value = {stat_tests['binomial_test']['p_value']:.4f}\")\n",
    "print(f\"   Significant at α=0.05: {stat_tests['binomial_test']['significant_at_0.05']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Judge Comparison\n",
    "\n",
    "If multiple judges were used, compare their preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judges = df[\"judge\"].unique()\n",
    "\n",
    "if len(judges) > 1:\n",
    "    # Create comparison matrix\n",
    "    comparison_data = []\n",
    "\n",
    "    for judge in judges:\n",
    "        top1_prefs = calculate_top1_preference(df, judge)\n",
    "        for _, row in top1_prefs.iterrows():\n",
    "            comparison_data.append(\n",
    "                {\"judge\": judge, \"vendor\": row[\"vendor\"], \"top1_percentage\": row[\"top1_percentage\"]}\n",
    "            )\n",
    "\n",
    "    comp_df = pd.DataFrame(comparison_data)\n",
    "    pivot = comp_df.pivot(index=\"judge\", columns=\"vendor\", values=\"top1_percentage\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        cmap=\"RdYlGn\",\n",
    "        center=33.33,\n",
    "        cbar_kws={\"label\": \"Top-1 Selection Rate (%)\"},\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Cross-Judge Comparison: Top-1 Preferences\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Model Vendor\", fontsize=12)\n",
    "    ax.set_ylabel(\"Judge\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data/results/cross_judge_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nCross-Judge Comparison:\")\n",
    "    print(pivot)\n",
    "else:\n",
    "    print(\"Only one judge used. Run with multiple judges for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Save analysis results to CSV files for further exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export main dataframe\n",
    "df.to_csv(\"data/results/analysis_data.csv\", index=False)\n",
    "print(\"✓ Main analysis data saved to data/results/analysis_data.csv\")\n",
    "\n",
    "# Export summary statistics\n",
    "target_judge = \"gemini_thinking\"\n",
    "top1_prefs = calculate_top1_preference(df, target_judge)\n",
    "top1_prefs.to_csv(\"data/results/top1_preferences.csv\", index=False)\n",
    "print(\"✓ Top-1 preferences saved to data/results/top1_preferences.csv\")\n",
    "\n",
    "scores_df = calculate_average_scores(df, target_judge)\n",
    "scores_df.to_csv(\"data/results/average_scores.csv\", index=False)\n",
    "print(\"✓ Average scores saved to data/results/average_scores.csv\")\n",
    "\n",
    "category_prefs = calculate_category_preference(df, target_judge)\n",
    "category_prefs.to_csv(\"data/results/category_preferences.csv\", index=False)\n",
    "print(\"✓ Category preferences saved to data/results/category_preferences.csv\")\n",
    "\n",
    "print(\"\\n✓ All results exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Based on the analysis above, summarize your findings:\n",
    "\n",
    "1. **Overall Bias**: Does Gemini show self-preference?\n",
    "2. **Tier Effects**: Does bias differ between Fast and Thinking tiers?\n",
    "3. **Category Effects**: Is bias stronger in subjective vs objective tasks?\n",
    "4. **Cross-Judge Comparison**: Do all judges show similar self-bias patterns?\n",
    "\n",
    "Add your interpretations and conclusions here after running the full analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
